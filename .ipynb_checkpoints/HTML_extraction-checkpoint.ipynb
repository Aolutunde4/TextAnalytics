{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1994f02c",
   "metadata": {},
   "source": [
    "# Extract HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87c84a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8735130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase that appears if we can't extract all html\n",
    "stop_phrase = \"Your browser does not support the audio element.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed38d5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 200 articles.\n",
      "Extracted 400 articles.\n",
      "Extracted 600 articles.\n",
      "Extracted 800 articles.\n",
      "Extracted 1000 articles.\n",
      "Extracted 1200 articles.\n",
      "Extracted 1400 articles.\n",
      "Extracted 1600 articles.\n",
      "Extracted 1800 articles.\n",
      "Extracted 2000 articles.\n",
      "Extracted 2200 articles.\n",
      "Extracted 2400 articles.\n",
      "Extracted 2600 articles.\n",
      "Extracted 2800 articles.\n",
      "Extracted 3000 articles.\n",
      "3198 total articles have been saved.\n"
     ]
    }
   ],
   "source": [
    "nPages = 128 # 128 pages gives about 3200 articles (25 per page)\n",
    "count_articles = 0 # count total number of articles we have\n",
    "\n",
    "# save article html\n",
    "article_html = []\n",
    "\n",
    "# iterate through url pages to get list of article HTML's\n",
    "for i in range(2, nPages + 2):\n",
    "    page_url = f\"https://www.nature.com/latest-news?page={i}\"\n",
    "    \n",
    "    # create soup object for each page\n",
    "    r = requests.get(page_url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    # get HTML of all articles on current page\n",
    "    page_html = soup.find_all(\"div\", {\"class\": \"c-article-item__content c-article-item--with-image\"})\n",
    "    \n",
    "    # iterate over each article\n",
    "    for article in page_html:\n",
    "        # In each 'a' tag, 'href' contains the article link\n",
    "        link = \"https://www.nature.com\" + article.find(\"a\")[\"href\"] \n",
    "        \n",
    "        # create soup object for each article\n",
    "        r2 = requests.get(link)\n",
    "        soup2 = BeautifulSoup(r2.content, \"html.parser\")\n",
    "        \n",
    "        # first h3 tag is the article title\n",
    "        title = article.find(\"h3\").text \n",
    "        \n",
    "        # append article title and html to list to write to txt file later\n",
    "        file_name = title.replace(\" \", \"_\") \n",
    "        article_html.appen((file_name, r2.content))\n",
    "        \n",
    "        # get date, check if missing\n",
    "        date_html = soup2.find_all(\"li\", {\"class\": \"c-article-identifiers__item\"})\n",
    "        date = date_html[-1].text if date_html != [] else None # last element is date\n",
    "        \n",
    "        # some authors are missing, check if None\n",
    "        author_html = soup2.find(\"li\", {\"class\": \"c-author-list__item\"})\n",
    "        author = author_html.find(\"a\").text if author_html is not None else None\n",
    "        \n",
    "        # get body of each article\n",
    "        body_html = soup2.find(\"div\", {\"class\": \"c-article-body u-clearfix\"})\n",
    "        # body_html will be None if article is in a different html format than usual\n",
    "        body = []\n",
    "        if body_html is not None:\n",
    "            for p in body_html.find_all(\"p\"):\n",
    "                # check if we could get html\n",
    "                if stop_phrase in p.text.strip():\n",
    "                    continue\n",
    "                else:\n",
    "                    body.append(p.text.strip())\n",
    "            str_body = \" \".join(body) # turn text into 1 string instead of list of sentences\n",
    "        \n",
    "        # write title, author, date, body to csv file \n",
    "        # use | as delimiter\n",
    "        with open(\"articles.txt\", \"a\") as fa:\n",
    "            fa.write(f\"{title}|{author}|{date}|{str_body}\\n\")  \n",
    "        \n",
    "        count_articles += 1\n",
    "        if count_articles % 200 == 0: print(f\"Extracted {count_articles} articles.\")\n",
    "        \n",
    "        # sleep for 1 second to not overload website\n",
    "        time.sleep(1)\n",
    "        \n",
    "print(f\"{count_articles} total articles have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73a201",
   "metadata": {},
   "source": [
    "# Write Article HTML to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in article_html:\n",
    "    title = article[0]\n",
    "    html = article[1]\n",
    "    with open(f\"project_htmls/{title}.html\", \"w+b\") as fw:\n",
    "        fw.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fbc00e",
   "metadata": {},
   "source": [
    "# Extract sentences from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8896fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted sentences from article 100.\n",
      "Extracted sentences from article 200.\n",
      "Extracted sentences from article 300.\n",
      "Extracted sentences from article 400.\n",
      "Extracted sentences from article 500.\n",
      "Extracted sentences from article 600.\n",
      "Extracted sentences from article 700.\n",
      "Extracted sentences from article 800.\n",
      "Extracted sentences from article 900.\n",
      "Extracted sentences from article 1000.\n",
      "Extracted sentences from article 1100.\n",
      "Extracted sentences from article 1200.\n",
      "Extracted sentences from article 1300.\n",
      "Extracted sentences from article 1400.\n",
      "Extracted sentences from article 1500.\n",
      "Extracted sentences from article 1600.\n",
      "Extracted sentences from article 1700.\n",
      "Extracted sentences from article 1800.\n",
      "Extracted sentences from article 1900.\n",
      "Extracted sentences from article 2000.\n",
      "Extracted sentences from article 2100.\n",
      "Extracted sentences from article 2200.\n",
      "Extracted sentences from article 2300.\n",
      "Extracted sentences from article 2400.\n",
      "Extracted sentences from article 2500.\n",
      "Extracted sentences from article 2600.\n",
      "Extracted sentences from article 2700.\n",
      "Extracted sentences from article 2800.\n",
      "Extracted sentences from article 2900.\n",
      "Extracted sentences from article 3000.\n",
      "Extracted sentences from article 3100.\n",
      "\n",
      "Extracted 156217 sentences.\n"
     ]
    }
   ],
   "source": [
    "### Get list of all sentences in all articles and save in a file ###\n",
    "import nltk\n",
    "\n",
    "# get all html files from folder\n",
    "html_files = [file for file in os.listdir(\"project_htmls\") if file.endswith(\".html\")]\n",
    "\n",
    "# iterate through all articles to extract text\n",
    "sentences = []\n",
    "count = 0\n",
    "for article in html_files:\n",
    "    # read html content of each article\n",
    "    with open(f\"project_htmls/{article}\", \"r+b\") as fr:\n",
    "        soup = BeautifulSoup(fr.read(), \"html.parser\")\n",
    "        \n",
    "        # get text of each article\n",
    "        body_html = soup.find(\"div\", {\"class\": \"c-article-body u-clearfix\"})\n",
    "        \n",
    "        # body will be None if article is in a different html format than usual\n",
    "        \n",
    "        if body_html is not None:\n",
    "            txt = []\n",
    "            for p in body_html.find_all(\"p\"):\n",
    "                # check if we could get html\n",
    "                if stop_phrase in p.text.strip():\n",
    "                    continue\n",
    "                else:\n",
    "                    txt.append(p.text.strip())\n",
    "            for paragraph in txt:\n",
    "                tokenized_sent = nltk.sent_tokenize(paragraph)\n",
    "                # tokenized_sent is a list of all sentences in the current article\n",
    "                # add sentences to sentences list\n",
    "                for sent in tokenized_sent:\n",
    "                    sentences.append(sent.strip()) # strip white space from sentences\n",
    "            \n",
    "            count += 1\n",
    "            if count % 100 == 0: print(f\"Extracted sentences from {count} articles.\")\n",
    "            \n",
    "\n",
    "# write all sentences to a file (each sentence on new line)\n",
    "with open(\"project_sentences.txt\", \"w\") as fw:\n",
    "    for s in sentences:\n",
    "        fw.write(f\"{s}\\n\")\n",
    "\n",
    "print(f\"\\nExtracted {len(sentences)} sentences from {count} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cd902",
   "metadata": {},
   "source": [
    "# Categorize sentences into positive, negative, and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb231c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### categorize sentences into positive, negative, and neutral ###\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# read all sentences\n",
    "with open(\"project_sentences.txt\", \"r\") as fr:\n",
    "    s = [line.strip() for line in fr]\n",
    "    \n",
    "    # perform sentiment analysis from TextBlob() for sampling purposes\n",
    "    df = pd.DataFrame({\"sentence\": s})\n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates(keep=\"first\")\n",
    "    \n",
    "    df[\"polarity\"] = df.sentence.apply(lambda x: TextBlob(x).polarity) # get polarity of each sentence\n",
    "    df[\"subjectivity\"] = df.sentence.apply(lambda x: TextBlob(x).subjectivity) # get subjectivity of each sentence\n",
    "    \n",
    "    # categorize into positive, negative, and neutral based on 0.5 polarity and above 0.6 subjectivity\n",
    "    # note: subjectivity allows us to get less neutral sentences\n",
    "    positive = df[(df.polarity >= 0.5) & (df.subjectivity >= 0.6)]\n",
    "    neutral = df[(-0.5 < df.polarity) & (df.polarity < 0.5) & (df.subjectivity >= 0.6)]\n",
    "    negative = df[(df.polarity <= -0.5) & (df.subjectivity >= 0.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bc5b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3644 1388 19300\n"
     ]
    }
   ],
   "source": [
    "print(len(positive.sentence),  len(negative.sentence), len(neutral.sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52bef9",
   "metadata": {},
   "source": [
    "## Sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b5f1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample 40% positive, 40% negative, and 20% netural for a total of 2000 sentences ###\n",
    "### Export sample sentences to Excel for labeling ###\n",
    "\n",
    "pos_sample = positive.sentence.sample(n=800, ignore_index=True, random_state=1)\n",
    "neg_sample = negative.sentence.sample(n=800, ignore_index=True, random_state=3)\n",
    "neutral_sample = neutral.sentence.sample(n=400, ignore_index=True, random_state=5)\n",
    "\n",
    "# combine dataframes and shuffle sentences\n",
    "sample_sentences = pd.concat([pos_sample, neg_sample, neutral_sample], ignore_index=True).sample(frac=1, ignore_index=True, random_state=10)\n",
    "\n",
    "# write to Excel file for manual labeling\n",
    "sample_sentences.to_excel(\"project_excel_sentences.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
