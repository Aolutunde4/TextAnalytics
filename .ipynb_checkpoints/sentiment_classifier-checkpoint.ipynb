{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3ef9fb",
   "metadata": {},
   "source": [
    "# Set Up Design Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457d6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.4585\n",
       "negative    0.3100\n",
       "positive    0.2315\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read labeled Excel file\n",
    "df = pd.read_excel(\"project_excel_sentences_labeled.xlsx\", usecols=\"B:C\", names=[\"sentences\", \"label\"])\n",
    "\n",
    "# get relative frequencies of positive, negative, and neutral labels\n",
    "df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a15d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 7055), (2000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# transform sentences into an inverse document frequency matrix\n",
    "# exclude terms that appear in more than 70% of our sentences\n",
    "vectorizer = TfidfVectorizer(use_idf=False, norm=\"l2\", stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# split into X and y\n",
    "X = vectorizer.fit_transform(df.sentences)\n",
    "y = df.label\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e3f89",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08476247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9813be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.441875\n",
       "negative    0.320625\n",
       "positive    0.237500\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to keep track of best results\n",
    "summary = {} \n",
    "\n",
    "# see distribution of y_train\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1895d6",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae045311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import pprint\n",
    "\n",
    "# parameters to try on KNN\n",
    "params = {\"n_neighbors\": [15, 17, 19, 21, 25, 30], \n",
    "          \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "# use grid search CV with various parameters to find optimal solution\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# get results from best knn parameters\n",
    "knn_results = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed98f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 30,\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.47      0.56      0.51       107\\n'\n",
      "           '     neutral       0.62      0.57      0.60       210\\n'\n",
      "           '    positive       0.33      0.31      0.32        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.52       400\\n'\n",
      "           '   macro avg       0.47      0.48      0.48       400\\n'\n",
      "           'weighted avg       0.52      0.52      0.52       400\\n',\n",
      " 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "# get predictions on test set\n",
    "pred_knn = grid.predict(X_test)\n",
    "\n",
    "# add results and classification report to summary\n",
    "knn_results[\"report\"] = classification_report(y_test, pred_knn)\n",
    "summary[\"knn\"] = knn_results\n",
    "pprint.pprint(knn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b557fa",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc871ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/bdnassif/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create LR parameters\n",
    "params = {\"C\": [5, 7, 9, 11],\n",
    "         \"penalty\": [\"l1\"],\n",
    "         \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# use grid search CV with various parameters to find optimal solution\n",
    "gridLR = GridSearchCV(LogisticRegression(max_iter=400), \n",
    "                      param_grid=params, \n",
    "                      cv=5)\n",
    "gridLR.fit(X_train, y_train)\n",
    "\n",
    "# get results from best LR parameters\n",
    "lr_results = gridLR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8a2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5,\n",
      " 'penalty': 'l1',\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.48      0.60      0.54       107\\n'\n",
      "           '     neutral       0.63      0.61      0.62       210\\n'\n",
      "           '    positive       0.45      0.35      0.39        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.55       400\\n'\n",
      "           '   macro avg       0.52      0.52      0.52       400\\n'\n",
      "           'weighted avg       0.55      0.55      0.55       400\\n',\n",
      " 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# classification report \n",
    "\n",
    "# get predictions\n",
    "pred_lr = gridLR.predict(X_test)\n",
    "\n",
    "# get results and add to summary\n",
    "lr_results[\"report\"] = classification_report(y_test, pred_lr)\n",
    "summary[\"lr\"] = lr_results\n",
    "pprint.pprint(lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a32f8",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04845a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create parameters for naive bayes\n",
    "params = {\"alpha\": [0.001, 0.01, 0.1, 1, 3]}\n",
    "\n",
    "gridMNB = GridSearchCV(MultinomialNB(), \n",
    "                       param_grid=params, \n",
    "                       cv=5)\n",
    "gridMNB.fit(X_train, y_train)\n",
    "\n",
    "# get results from MNB\n",
    "mnb_results = gridMNB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccee5375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1,\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.48      0.56      0.52       107\\n'\n",
      "           '     neutral       0.62      0.59      0.60       210\\n'\n",
      "           '    positive       0.39      0.37      0.38        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.54       400\\n'\n",
      "           '   macro avg       0.50      0.51      0.50       400\\n'\n",
      "           'weighted avg       0.54      0.54      0.54       400\\n'}\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "pred_mnb = gridMNB.predict(X_test)\n",
    "\n",
    "# get results\n",
    "mnb_results[\"report\"] = classification_report(y_test, pred_mnb)\n",
    "summary[\"mnb\"] = mnb_results\n",
    "pprint.pprint(mnb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2772628",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58b5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create parameters for random forest\n",
    "params = {\"min_samples_split\": [3, 5, 10, 15],\n",
    "         \"criterion\": [\"entropy\"]}\n",
    "\n",
    "gridRF = GridSearchCV(RandomForestClassifier(n_estimators=400, max_depth=None), \n",
    "                      param_grid=params, \n",
    "                      cv=5)\n",
    "gridRF.fit(X_train, y_train)\n",
    "\n",
    "# get results from RF\n",
    "rf_results = gridRF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeae0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy',\n",
      " 'min_samples_split': 15,\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.51      0.45      0.48       107\\n'\n",
      "           '     neutral       0.60      0.78      0.68       210\\n'\n",
      "           '    positive       0.68      0.28      0.39        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.59       400\\n'\n",
      "           '   macro avg       0.60      0.50      0.52       400\\n'\n",
      "           'weighted avg       0.59      0.59      0.57       400\\n'}\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "pred_rf = gridRF.predict(X_test)\n",
    "\n",
    "# get results and add to summary\n",
    "rf_results[\"report\"] = classification_report(y_test, pred_rf)\n",
    "summary[\"rf\"] = rf_results\n",
    "pprint.pprint(rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9eb87e",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d7f454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# create parameters for linear SVM\n",
    "params = {\"C\": [0.1, 0.5, 1, 5, 10],\n",
    "         \"penalty\": [\"l1\", \"l2\"],\n",
    "         \"loss\": [\"squared_hinge\"]}\n",
    "\n",
    "gridSVM = GridSearchCV(LinearSVC(dual=False, tol=0.001),\n",
    "                      param_grid=params,\n",
    "                      cv=5)\n",
    "gridSVM.fit(X_train, y_train)\n",
    "\n",
    "# get results from linear SVM\n",
    "svm_results = gridSVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9400e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5,\n",
      " 'loss': 'squared_hinge',\n",
      " 'penalty': 'l1',\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.54      0.55      0.55       107\\n'\n",
      "           '     neutral       0.63      0.72      0.67       210\\n'\n",
      "           '    positive       0.54      0.31      0.40        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.59       400\\n'\n",
      "           '   macro avg       0.57      0.53      0.54       400\\n'\n",
      "           'weighted avg       0.59      0.59      0.58       400\\n'}\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "pred_svm = gridSVM.predict(X_test)\n",
    "\n",
    "# get results and add to summary\n",
    "svm_results[\"report\"] = classification_report(y_test, pred_svm)\n",
    "summary[\"linearSVM\"] = svm_results\n",
    "pprint.pprint(svm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edcdfeb",
   "metadata": {},
   "source": [
    "## Kernalized SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0429b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# specify parameters for SVC\n",
    "params = {\"C\": [1, 5, 7, 9],\n",
    "         \"kernel\": [\"rbf\", \"poly\", \"linear\", \"sigmoid\"]}\n",
    "\n",
    "gridSVM = GridSearchCV(SVC(),\n",
    "                      param_grid=params,\n",
    "                      cv=5)\n",
    "gridSVM.fit(X_train, y_train)\n",
    "\n",
    "# get best parameters from SVM\n",
    "svm_results = gridSVM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7149f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1,\n",
      " 'kernel': 'linear',\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.52      0.55      0.54       107\\n'\n",
      "           '     neutral       0.61      0.69      0.65       210\\n'\n",
      "           '    positive       0.51      0.31      0.39        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.57       400\\n'\n",
      "           '   macro avg       0.55      0.52      0.52       400\\n'\n",
      "           'weighted avg       0.57      0.57      0.57       400\\n'}\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "pred_ker_svm = gridSVM.predict(X_test)\n",
    "\n",
    "# get results and add to summary\n",
    "svm_results[\"report\"] = classification_report(y_test, pred_ker_svm)\n",
    "summary[\"SVM\"] = svm_results\n",
    "pprint.pprint(svm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d21c7c",
   "metadata": {},
   "source": [
    "## Neural Network - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bce152a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# specify parameters to try\n",
    "#### long training time. The parameters below were tested. `optimal_params` below provide the best results. ####\n",
    "# params = {\"hidden_layer_sizes\": [(10, ), (20, ), (30, )],\n",
    "#          \"activation\": [\"relu\", \"identity\", \"logistic\", \"tanh\"],\n",
    "#          \"solver\": [\"adam\", \"sgd\", \"lbfgs\"],\n",
    "#          \"alpha\": [0.00001, 0.0001, 0.01, 1]}\n",
    "\n",
    "optimal_params = {\"hidden_layer_sizes\": [(30, )],\n",
    "         \"activation\": [\"identity\"],\n",
    "         \"solver\": [\"lbfgs\"],\n",
    "         \"alpha\": [0.0001]}\n",
    "\n",
    "gridMLP = GridSearchCV(MLPClassifier(max_iter=300), \n",
    "                      param_grid=optimal_params,\n",
    "                      cv=5)\n",
    "gridMLP.fit(X_train, y_train)\n",
    "\n",
    "# get best parameters from MLP\n",
    "mlp_results = gridMLP.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4b7d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'identity',\n",
      " 'alpha': 0.0001,\n",
      " 'hidden_layer_sizes': (30,),\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.56      0.52      0.54       107\\n'\n",
      "           '     neutral       0.63      0.70      0.66       210\\n'\n",
      "           '    positive       0.43      0.35      0.38        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.58       400\\n'\n",
      "           '   macro avg       0.54      0.52      0.53       400\\n'\n",
      "           'weighted avg       0.57      0.58      0.57       400\\n',\n",
      " 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "pred_mlp = gridMLP.predict(X_test)\n",
    "\n",
    "# get results and add to summary\n",
    "mlp_results[\"report\"] = classification_report(y_test, pred_mlp)\n",
    "summary[\"MLP\"] = mlp_results\n",
    "pprint.pprint(mlp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471552b",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9814874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# parameters to be tested\n",
    "params = {\"max_depth\": [3, 7, 11]}\n",
    "\n",
    "gridGB = GridSearchCV(GradientBoostingClassifier(n_estimators=400, learning_rate=0.1), \n",
    "                     param_grid=params,\n",
    "                     cv=5)\n",
    "gridGB.fit(X_train, y_train)\n",
    "\n",
    "# get best parameters\n",
    "gb_results = gridGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d9adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 7,\n",
      " 'report': '              precision    recall  f1-score   support\\n'\n",
      "           '\\n'\n",
      "           '    negative       0.49      0.56      0.52       107\\n'\n",
      "           '     neutral       0.65      0.68      0.67       210\\n'\n",
      "           '    positive       0.61      0.43      0.51        83\\n'\n",
      "           '\\n'\n",
      "           '    accuracy                           0.60       400\\n'\n",
      "           '   macro avg       0.58      0.56      0.57       400\\n'\n",
      "           'weighted avg       0.60      0.60      0.60       400\\n'}\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "pred_gb = gridGB.predict(X_test)\n",
    "\n",
    "# get results and add to summary\n",
    "gb_results[\"report\"] = classification_report(y_test, pred_gb)\n",
    "summary[\"GB\"] = gb_results\n",
    "pprint.pprint(gb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81224a3b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c1a0026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.56      0.51       107\n",
      "     neutral       0.62      0.57      0.60       210\n",
      "    positive       0.33      0.31      0.32        83\n",
      "\n",
      "    accuracy                           0.52       400\n",
      "   macro avg       0.47      0.48      0.48       400\n",
      "weighted avg       0.52      0.52      0.52       400\n",
      "\n",
      "lr:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.60      0.54       107\n",
      "     neutral       0.63      0.61      0.62       210\n",
      "    positive       0.45      0.35      0.39        83\n",
      "\n",
      "    accuracy                           0.55       400\n",
      "   macro avg       0.52      0.52      0.52       400\n",
      "weighted avg       0.55      0.55      0.55       400\n",
      "\n",
      "mnb:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.56      0.52       107\n",
      "     neutral       0.62      0.59      0.60       210\n",
      "    positive       0.39      0.37      0.38        83\n",
      "\n",
      "    accuracy                           0.54       400\n",
      "   macro avg       0.50      0.51      0.50       400\n",
      "weighted avg       0.54      0.54      0.54       400\n",
      "\n",
      "rf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.45      0.48       107\n",
      "     neutral       0.60      0.78      0.68       210\n",
      "    positive       0.68      0.28      0.39        83\n",
      "\n",
      "    accuracy                           0.59       400\n",
      "   macro avg       0.60      0.50      0.52       400\n",
      "weighted avg       0.59      0.59      0.57       400\n",
      "\n",
      "linearSVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.55      0.55       107\n",
      "     neutral       0.63      0.72      0.67       210\n",
      "    positive       0.54      0.31      0.40        83\n",
      "\n",
      "    accuracy                           0.59       400\n",
      "   macro avg       0.57      0.53      0.54       400\n",
      "weighted avg       0.59      0.59      0.58       400\n",
      "\n",
      "SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.55      0.54       107\n",
      "     neutral       0.61      0.69      0.65       210\n",
      "    positive       0.51      0.31      0.39        83\n",
      "\n",
      "    accuracy                           0.57       400\n",
      "   macro avg       0.55      0.52      0.52       400\n",
      "weighted avg       0.57      0.57      0.57       400\n",
      "\n",
      "MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.52      0.54       107\n",
      "     neutral       0.63      0.70      0.66       210\n",
      "    positive       0.43      0.35      0.38        83\n",
      "\n",
      "    accuracy                           0.58       400\n",
      "   macro avg       0.54      0.52      0.53       400\n",
      "weighted avg       0.57      0.58      0.57       400\n",
      "\n",
      "GB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.56      0.52       107\n",
      "     neutral       0.65      0.68      0.67       210\n",
      "    positive       0.61      0.43      0.51        83\n",
      "\n",
      "    accuracy                           0.60       400\n",
      "   macro avg       0.58      0.56      0.57       400\n",
      "weighted avg       0.60      0.60      0.60       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in summary:\n",
    "    print(f\"{model}:\\n{summary[model]['report']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e41670",
   "metadata": {},
   "source": [
    "Gradient Boosting achieved the best accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cfb8a",
   "metadata": {},
   "source": [
    "# Test on our own sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d1cdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\"We are hoping this is a happy and nice sentence\",\n",
    "                 \"I want this to be a bad and gross sentence\",\n",
    "                 \"There is great weather outside today\",\n",
    "                 \"The weather is dull and rainy outside\",\n",
    "                 \"We are using Python for machine learning\",\n",
    "                 \"The summer is my favorite time of year\",\n",
    "                 \"We learned many new topics in this course\",\n",
    "                 \"I'm very excited to play with my dog today\",\n",
    "                 \"I am annoyed at my bad performance\",\n",
    "                 \"The date is May 3, 2022\"]\n",
    "hand_labels = [\"positive\", \"negative\", \"positive\", \"negative\", \"neutral\", \"positive\", \n",
    "               \"neutral\", \"positive\", \"negative\", \"neutral\"]\n",
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e4ba67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x7055 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use vectorizer to transform sentences into tfidf matrix\n",
    "testX = vectorizer.transform(test_sentences)\n",
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c3e270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>hand_labeled</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are hoping this is a happy and nice sentence</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want this to be a bad and gross sentence</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is great weather outside today</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The weather is dull and rainy outside</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We are using Python for machine learning</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The summer is my favorite time of year</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We learned many new topics in this course</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm very excited to play with my dog today</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I am annoyed at my bad performance</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The date is May 3, 2022</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sentences hand_labeled Predictions\n",
       "0  We are hoping this is a happy and nice sentence     positive    positive\n",
       "1       I want this to be a bad and gross sentence     negative    negative\n",
       "2             There is great weather outside today     positive    positive\n",
       "3            The weather is dull and rainy outside     negative     neutral\n",
       "4         We are using Python for machine learning      neutral     neutral\n",
       "5           The summer is my favorite time of year     positive     neutral\n",
       "6        We learned many new topics in this course      neutral     neutral\n",
       "7       I'm very excited to play with my dog today     positive     neutral\n",
       "8               I am annoyed at my bad performance     negative    negative\n",
       "9                          The date is May 3, 2022      neutral     neutral"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use gradient boosting to predict polarity of sentences\n",
    "pred_testX = gridGB.predict(testX)\n",
    "\n",
    "# view predictions\n",
    "test_df = pd.DataFrame({\"Sentences\": test_sentences, \"hand_labeled\": hand_labels, \"Predictions\": pred_testX})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00bed3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80         3\n",
      "     neutral       0.50      1.00      0.67         3\n",
      "    positive       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.83      0.72      0.71        10\n",
      "weighted avg       0.85      0.70      0.71        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.hand_labeled, test_df.Predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00850e",
   "metadata": {},
   "source": [
    "## Comparison to Textblob Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c45a680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>hand_labeled</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are hoping this is a happy and nice sentence</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want this to be a bad and gross sentence</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is great weather outside today</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The weather is dull and rainy outside</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We are using Python for machine learning</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The summer is my favorite time of year</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We learned many new topics in this course</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm very excited to play with my dog today</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I am annoyed at my bad performance</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The date is May 3, 2022</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sentences hand_labeled Predictions  \\\n",
       "0  We are hoping this is a happy and nice sentence     positive    positive   \n",
       "1       I want this to be a bad and gross sentence     negative    negative   \n",
       "2             There is great weather outside today     positive    positive   \n",
       "3            The weather is dull and rainy outside     negative     neutral   \n",
       "4         We are using Python for machine learning      neutral     neutral   \n",
       "5           The summer is my favorite time of year     positive     neutral   \n",
       "6        We learned many new topics in this course      neutral     neutral   \n",
       "7       I'm very excited to play with my dog today     positive     neutral   \n",
       "8               I am annoyed at my bad performance     negative    negative   \n",
       "9                          The date is May 3, 2022      neutral     neutral   \n",
       "\n",
       "   textblob  \n",
       "0  positive  \n",
       "1   neutral  \n",
       "2   neutral  \n",
       "3   neutral  \n",
       "4   neutral  \n",
       "5  positive  \n",
       "6   neutral  \n",
       "7   neutral  \n",
       "8  negative  \n",
       "9   neutral  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# use TextBlob to get polarity\n",
    "def textblob_label(sentence):\n",
    "    polarity = TextBlob(sentence).polarity\n",
    "    # use 0.5 thresholds for positive, negative, and neutral\n",
    "    if polarity >= 0.5:\n",
    "        return \"positive\"\n",
    "    elif polarity <= -0.5:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# add textblob labels to df\n",
    "test_df[\"textblob\"] = test_df.Sentences.apply(lambda x: textblob_label(x)) \n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d443d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.33      0.50         3\n",
      "     neutral       0.43      1.00      0.60         3\n",
      "    positive       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.81      0.61      0.59        10\n",
      "weighted avg       0.83      0.60      0.60        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TextBlob results\n",
    "print(classification_report(test_df.hand_labeled, test_df.textblob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
